# Comparative Neuroâ€‘evolution  
## **PPO vs. CMAâ€‘ES on MuJoCo Continuousâ€‘Control Tasks**

> **Tasks:** `Reacherâ€‘v5`, `Antâ€‘v5`, `HalfCheetahâ€‘v5`, `InvertedDoublePendulumâ€‘v5`  
> **Algorithms:** Proximal Policy Optimization (**PPO**, gradientâ€‘based) **vs.** Covariance Matrix Adaptation Evolution Strategy (**CMAâ€‘ES**, gradientâ€‘free)

---

### Key Takeâ€‘aways
* **PPO** converges faster and is more sampleâ€‘efficient.
* **CMAâ€‘ES** better explores rugged reward landscapes.
* Both reach competitive final returns across all four tasks.

---

## Table of Contents
1. [Project Overview](#project-overview)  
2. [Repository Layout](#repository-layout)  
3. [Installation & Environment](#installation--environment)  
4. [Reproducing Results](#reproducing-results)  
   * [CMAâ€‘ES](#cma-es)  
   * [PPO](#ppo)  
   * [Running on HPCÂ /Â Slurm](#running-on-hpc--slurm)  
5. [Results](#results)  
6. [Videos](#videos)  
7. [Plots & Visualization](#plots--visualization)  
8. [Notes & Credits](#notes--credits)

---

## Project Overview
This study benchmarks a policyâ€‘gradient RL method (**PPO**) against a populationâ€‘based evolutionary method (**CMAâ€‘ES**) on four MuJoCo continuousâ€‘control environments. We provide:

* **Reproducible code** (PythonÂ 3.8)
* **Configurable experiments** via YAML
* **CSV logs** of rewards / fitness
* **Rollout videos** of best policies
* **Automated plots** for learning curves and finalâ€‘score distributions

---

## Repository Layout
```text
.
â”œâ”€â”€ README.md                â† this file
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ cma_es_project/      â† CMAâ€‘ES implementation
â”‚   â”‚   â”œâ”€â”€ config/          â† YAML configs per task
â”‚   â”‚   â”œâ”€â”€ neural_network/
â”‚   â”‚   â”œâ”€â”€ optimization/
â”‚   â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â””â”€â”€ ppo/                 â† PPO Jupyter notebooks
â”‚
â”œâ”€â”€ scripts/                 â† Slurm job scripts
â”œâ”€â”€ tests/                   â† unit tests
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cma_es_runs/         â† CSV logs (CMAâ€‘ES)
â”‚   â””â”€â”€ ppo_runs/            â† CSV logs (PPO)
â”‚
â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ cma_es/              â† rollout MP4s
â”‚   â””â”€â”€ ppo/
â”‚
â””â”€â”€ plots/                   â† preâ€‘generated PNGs
```

---

## Installation & Environment
```bash
# clone the repo
git clone https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo.git
cd PPO-vs-CMAES-MuJoCo

# create & activate virtualenv
python -m venv venv
source venv/bin/activate        # on Windows: venv\Scripts\activate

# install Python deps
pip install -r requirements.txt
```

### MuJoCoÂ 3.2.6
1. Download MuJoCoÂ 3.2.6 from <https://mujoco.org>.
2. Add environment variables (adjust paths):
   ```bash
   export MUJOCO_PY_MUJOCO_PATH=/path/to/mujoco-3.2.6
   export LD_LIBRARY_PATH=/path/to/mujoco-3.2.6/bin:/path/to/glfw-3.3.8/src:$LD_LIBRARY_PATH
   export MUJOCO_GL=egl   # use 'osmesa' if EGL is unavailable
   ```
3. `gymnasium[mujoco]` is installed through `requirements.txt`.

---

## Reproducing Results
### CMAâ€‘ES
```bash
cd code/cma_es_project

# train (example: Reacherâ€‘v5)
python -m main \
  --mode train \
  --config config/test.yaml \
  --seed 505
```

ConfigÂ files shipped in `code/cma_es_project/config/`:

| Task | Config | population_size | sigma |
|------|--------|-----------------|-------|
| Reacherâ€‘v5 | `test.yaml` | 50 | 0.3 |
| Antâ€‘v5 | `test11.yaml` | 50 | 0.1 |
| HalfCheetahâ€‘v5 | `cma_es_2.yaml` | 25 | 0.2 |
| InvertedDoublePendulumâ€‘v5 | `test2.yaml` | 50 | 0.3 |

CSV logs â†’ `data/cma_es_runs/<task>/rewards.csv`

**Evaluate best policy**
```bash
python -m main \
  --mode evaluate \
  --config config/test.yaml \
  --checkpoint logs/checkpoints/best_model.pth \
  --seed 505
```

### PPO
```bash
cd code/ppo
jupyter notebook
```
Run notebooks:
* `Reacher_PPO.ipynb`
* `Ant_PPO.ipynb`
* `Halfcheetah_PPO.ipynb`
* `IDP_PPO.ipynb`

Logs saved to `data/ppo_runs/<task>/â€¦`

*(Optional) convert notebooks to scripts:*
```bash
pip install nbconvert
jupyter nbconvert --to script *.ipynb
python ppo_reacher.py
```

### Running on HPCÂ /Â Slurm
Adapt `code/scripts/*.sh` to your paths, then:
```bash
sbatch code/scripts/test5.sh
```

---

## Results
| Task | PPOÂ Return â†‘ | CMAâ€‘ESÂ Return |
|------|--------------|---------------|
| Reacherâ€‘v5 | **â‰ˆÂ âˆ’5** | âˆ’15 |
| Antâ€‘v5 | **â‰ˆÂ 2000** | 1800 |
| HalfCheetahâ€‘v5 | **â‰ˆÂ 5500â€“6000** | 2500â€“3000 |
| InvertedDoublePendulumâ€‘v5 | **â‰ˆÂ 9000** | 6000 |

---

## Videos
| Task | PPO | CMAâ€‘ES |
|------|-----|--------|
| Reacherâ€‘v5 | `videos/ppo/reacher.mp4` | `videos/cma_es/reacher.mp4` |
| Antâ€‘v5 | `videos/ppo/Ant.mp4` | `videos/cma_es/Ant.mp4` |
| HalfCheetahâ€‘v5 | `videos/ppo/HalfCheetah.mp4` | `videos/cma_es/HalfCheetha.mp4` |
| InvertedDoublePendulumâ€‘v5 | `videos/ppo/IDP.mp4` | `videos/cma_es/IDP.mp4` |

> **Large MP4s** are mirrored on GoogleÂ Drive â€“ see link in the paper.

---

## Plots & Visualization
* Preâ€‘generated PNGs live in `plots/<task>/`.
* Reâ€‘generate CMAâ€‘ES learning curves:
  ```bash
  python code/cma_es_project/utils/plotter.py --run_dir data/cma_es_runs/Reacher
  ```
* PPO curves are produced inside notebooks.

---

## Notes & Credits
* Large checkpoints (`best_model.pth`, `train_results.npy`) are excluded from version control.
* Experiments used MuJoCoÂ 3.2.6 + PythonÂ 3.8.
* Slurm scripts assume an EGLâ€‘enabled cluster; tweak for local runs.

### Citation
```
Author (2025). Comparative Neuroâ€‘evolution: PPO vs. CMAâ€‘ES in MuJoCo Environments. GitHub. https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo
```

Released under the MIT License â€“ see [LICENSE](LICENSE).

---

*Happy experimentingÂ ğŸš€*

